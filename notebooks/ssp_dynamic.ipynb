{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Shortest Path with Dynamic Programming\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pedronahum/stochastic-optimization/blob/master/notebooks/ssp_dynamic.ipynb)\n",
    "\n",
    "## Problem Overview\n",
    "\n",
    "This notebook demonstrates stochastic shortest path optimization using JAX. The problem involves:\n",
    "- **Random graph navigation** with uncertain edge costs\n",
    "- **Multi-step lookahead** using backward induction\n",
    "- **Running average cost estimation** - learning edge costs over time\n",
    "- **Risk-sensitive decision making** using percentile-based policies\n",
    "\n",
    "The agent must navigate from a starting node to a target node while:\n",
    "- Minimizing total path cost\n",
    "- Learning edge costs through experience\n",
    "- Planning ahead with dynamic programming\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### State Space\n",
    "The state at time $t$ is:\n",
    "$$s_t = [\\text{node}_t, t, \\hat{C}, N]$$\n",
    "\n",
    "where:\n",
    "- $\\text{node}_t \\in \\{0, 1, \\ldots, n-1\\}$ is the current node\n",
    "- $t$ is the current time step\n",
    "- $\\hat{C} \\in \\mathbb{R}^{n \\times n}$ is the matrix of estimated edge costs\n",
    "- $N \\in \\mathbb{N}^{n \\times n}$ is the observation count matrix (for running averages)\n",
    "\n",
    "### Graph Structure\n",
    "The graph is represented by:\n",
    "- **Adjacency matrix** $A \\in \\{0,1\\}^{n \\times n}$: $A[i,j] = 1$ if edge $(i,j)$ exists\n",
    "- **Mean costs** $\\mu \\in \\mathbb{R}_+^{n \\times n}$: Expected cost for each edge\n",
    "- **Spreads** $\\sigma \\in [0,1)^{n \\times n}$: Relative variability of edge costs\n",
    "\n",
    "### Dynamics\n",
    "When the agent moves from node $i$ to node $j$:\n",
    "\n",
    "1. **Sample cost**: $c_{i,j} \\sim \\text{Uniform}[\\mu_{i,j}(1-\\sigma_{i,j}), \\mu_{i,j}(1+\\sigma_{i,j})]$\n",
    "2. **Update estimate** (running average):\n",
    "   $$\\hat{C}[i,j] \\leftarrow \\left(1 - \\frac{1}{N[i,j]}\\right) \\hat{C}[i,j] + \\frac{1}{N[i,j]} c_{i,j}$$\n",
    "3. **Increment observations**: $N[i,j] \\leftarrow N[i,j] + 1$\n",
    "4. **Move**: $\\text{node}_{t+1} = j$, $t \\leftarrow t + 1$\n",
    "\n",
    "### Reward Function\n",
    "The single-step reward is the negative edge cost:\n",
    "$$R(s_t, a_t, w_t) = -c_{i,j}$$\n",
    "\n",
    "where $a_t = j$ is the decision to move to node $j$.\n",
    "\n",
    "### Policy: Lookahead with Backward Induction\n",
    "\n",
    "The **LookaheadPolicy** uses dynamic programming over horizon $H$:\n",
    "\n",
    "1. **Value function initialization** (terminal condition):\n",
    "   $$V_H[k] = \\begin{cases} 0 & \\text{if } k = \\text{target} \\\\ \\infty & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "2. **Backward induction** for $t = H-1, H-2, \\ldots, 0$:\n",
    "   $$V_t[i] = \\min_{j : A[i,j]=1} \\left\\{ \\tilde{C}_{\\theta}[i,j] + V_{t+1}[j] \\right\\}$$\n",
    "\n",
    "3. **Risk-adjusted costs** (percentile-based):\n",
    "   $$\\tilde{C}_{\\theta}[i,j] = \\hat{C}[i,j] \\cdot \\left[(1 - \\sigma[i,j]) + 2\\sigma[i,j] \\cdot \\theta\\right]$$\n",
    "   \n",
    "   - $\\theta = 0.0$: Pessimistic (uses $(1-\\sigma)\\mu$, assumes worst case)\n",
    "   - $\\theta = 0.5$: Risk-neutral (uses $\\mu$, expected value)\n",
    "   - $\\theta = 1.0$: Optimistic (uses $(1+\\sigma)\\mu$, assumes best case)\n",
    "\n",
    "4. **Decision**:\n",
    "   $$a^*_t = \\arg\\min_{j : A[i,j]=1} \\left\\{ \\tilde{C}_{\\theta}[i,j] + V_0[j] \\right\\}$$\n",
    "\n",
    "### Objective\n",
    "Minimize expected total cost to reach target:\n",
    "$$\\min_{\\pi} \\mathbb{E}\\left[\\sum_{t=0}^{T-1} c_{\\text{node}_t, a_t}\\right]$$\n",
    "\n",
    "where $T$ is the (random) time when the target is reached.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages and clone the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX and dependencies",
    "!pip install -q jax jaxlib jaxtyping chex numpy matplotlib networkx",
    "",
    "# Clone repository (only needed for Colab)",
    "import os",
    "if 'COLAB_GPU' in os.environ or not os.path.exists('problems'):",
    "    !git clone https://github.com/pedronahum/stochastic-optimization.git",
    "    os.chdir('stochastic-optimization')",
    "",
    "# Clear Python import cache to ensure latest code is loaded",
    "import sys",
    "for key in list(sys.modules.keys()):",
    "    if key.startswith('problems'):",
    "        del sys.modules[key]",
    "",
    "print(\"\u2713 Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from typing import List, Dict\n",
    "\n",
    "# Import problem components\n",
    "from problems.ssp_dynamic import (\n",
    "    SSPDynamicConfig,\n",
    "    SSPDynamicModel,\n",
    "    ExogenousInfo,\n",
    "    LookaheadPolicy,\n",
    "    GreedyLookaheadPolicy,\n",
    "    RandomPolicy,\n",
    ")\n",
    "\n",
    "print(\"\u2713 Imports successful\")\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX backend: {jax.default_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Configuration\n",
    "\n",
    "Let's set up the stochastic shortest path parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configuration\n",
    "config = SSPDynamicConfig(\n",
    "    n_nodes=10,          # 10 nodes in the graph\n",
    "    horizon=15,          # 15-step lookahead for dynamic programming\n",
    "    edge_prob=0.3,       # 30% probability of edge existing\n",
    "    cost_min=1.0,        # Minimum edge cost\n",
    "    cost_max=10.0,       # Maximum edge cost\n",
    "    max_spread=0.3,      # Maximum 30% spread around mean cost\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Nodes: {config.n_nodes}\")\n",
    "print(f\"  Lookahead horizon: {config.horizon}\")\n",
    "print(f\"  Edge probability: {config.edge_prob:.1%}\")\n",
    "print(f\"  Cost range: [{config.cost_min}, {config.cost_max}]\")\n",
    "print(f\"  Max spread: \u00b1{config.max_spread:.1%}\")\n",
    "print(f\"  State size: {2 + 2 * config.n_nodes * config.n_nodes} (node + time + costs + counts)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model and Policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = SSPDynamicModel(config)\n",
    "\n",
    "# Create policies\n",
    "lookahead_policy = LookaheadPolicy(theta=0.5)  # Risk-neutral\n",
    "greedy_policy = GreedyLookaheadPolicy()        # Single-step lookahead\n",
    "random_policy = RandomPolicy()                 # Random baseline\n",
    "\n",
    "print(f\"\u2713 Model initialized\")\n",
    "print(f\"  Target node: {model.target_node}\")\n",
    "print(f\"  Total edges: {int(jnp.sum(model.adjacency))}\")\n",
    "print(f\"  Average degree: {jnp.sum(model.adjacency) / config.n_nodes:.1f}\")\n",
    "print(f\"\\n\u2713 Policies created:\")\n",
    "print(f\"  - LookaheadPolicy (\u03b8=0.5, risk-neutral, {config.horizon}-step backward induction)\")\n",
    "print(f\"  - GreedyLookaheadPolicy (single-step, uses estimated costs)\")\n",
    "print(f\"  - RandomPolicy (baseline)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Graph Structure\n",
    "\n",
    "Let's visualize the directed graph with edge costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_graph(model, show_costs=True):\n",
    "    \"\"\"Visualize the graph structure with NetworkX.\"\"\"\n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for i in range(model.config.n_nodes):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add edges with mean costs\n",
    "    edge_labels = {}\n",
    "    for i in range(model.config.n_nodes):\n",
    "        for j in range(model.config.n_nodes):\n",
    "            if model.adjacency[i, j]:\n",
    "                cost = float(model.mean_costs[i, j])\n",
    "                spread = float(model.spreads[i, j])\n",
    "                G.add_edge(i, j, weight=cost)\n",
    "                if show_costs:\n",
    "                    edge_labels[(i, j)] = f\"{cost:.1f}\u00b1{spread*100:.0f}%\"\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, seed=42, k=2, iterations=50)\n",
    "    \n",
    "    # Draw\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Color nodes: start (green), target (red), others (lightblue)\n",
    "    node_colors = ['lightblue'] * model.config.n_nodes\n",
    "    node_colors[0] = 'lightgreen'\n",
    "    node_colors[model.target_node] = 'lightcoral'\n",
    "    \n",
    "    # Draw nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                           node_size=800, ax=ax, alpha=0.9)\n",
    "    \n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', \n",
    "                           arrows=True, arrowsize=20, \n",
    "                           arrowstyle='->', ax=ax, alpha=0.6,\n",
    "                           connectionstyle='arc3,rad=0.1')\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=14, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Draw edge labels (costs)\n",
    "    if show_costs:\n",
    "        nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8, ax=ax)\n",
    "    \n",
    "    ax.set_title(f'Graph Structure (Node 0 \u2192 Node {model.target_node})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightgreen', label='Start (Node 0)'),\n",
    "        Patch(facecolor='lightcoral', label=f'Target (Node {model.target_node})'),\n",
    "        Patch(facecolor='lightblue', label='Intermediate Nodes')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nGraph Statistics:\")\n",
    "    print(f\"  Total edges: {G.number_of_edges()}\")\n",
    "    print(f\"  Average out-degree: {G.number_of_edges() / G.number_of_nodes():.2f}\")\n",
    "    print(f\"  Is connected: {nx.is_weakly_connected(G)}\")\n",
    "\n",
    "visualize_graph(model, show_costs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Simulation\n",
    "\n",
    "Let's simulate path finding until the agent reaches the target node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(model, policy, max_steps=50, key=None, verbose=True):\n",
    "    \"\"\"Run a single episode until target is reached or max steps.\"\"\"\n",
    "    if key is None:\n",
    "        key = jax.random.PRNGKey(42)\n",
    "    \n",
    "    # Initialize\n",
    "    key, subkey = jax.random.split(key)\n",
    "    state = model.init_state(subkey)\n",
    "    \n",
    "    # Track history\n",
    "    history = {\n",
    "        'path': [0],  # Start at node 0\n",
    "        'costs': [],\n",
    "        'cumulative_cost': [],\n",
    "        'states': [state],\n",
    "    }\n",
    "    \n",
    "    total_cost = 0.0\n",
    "    \n",
    "    # Run simulation\n",
    "    for t in range(max_steps):\n",
    "        # Check if reached target\n",
    "        if model.is_terminal(state):\n",
    "            if verbose:\n",
    "                print(f\"\u2713 Reached target node {model.target_node} in {t} steps!\")\n",
    "            break\n",
    "        \n",
    "        # Get decision\n",
    "        key, subkey = jax.random.split(key)\n",
    "        decision = policy(None, state, subkey, model)\n",
    "        \n",
    "        # Sample exogenous events\n",
    "        key, subkey = jax.random.split(key)\n",
    "        exog = model.sample_exogenous(subkey, state, t)\n",
    "        \n",
    "        # Compute reward (negative cost)\n",
    "        reward = model.reward(state, decision, exog)\n",
    "        cost = -float(reward)\n",
    "        total_cost += cost\n",
    "        \n",
    "        # Record\n",
    "        history['path'].append(int(decision))\n",
    "        history['costs'].append(cost)\n",
    "        history['cumulative_cost'].append(total_cost)\n",
    "        \n",
    "        # Transition\n",
    "        state = model.transition(state, decision, exog)\n",
    "        history['states'].append(state)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f\"\u26a0 Did not reach target in {max_steps} steps\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  Total cost: {total_cost:.2f}\")\n",
    "        print(f\"  Path: {' \u2192 '.join(map(str, history['path']))}\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Run simulation with LookaheadPolicy\n",
    "print(\"Running simulation with LookaheadPolicy (\u03b8=0.5)...\\n\")\n",
    "key = jax.random.PRNGKey(42)\n",
    "history = run_episode(model, lookahead_policy, max_steps=50, key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Path Taken\n",
    "\n",
    "Let's visualize the path the agent took through the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_path(model, history):\n",
    "    \"\"\"Visualize the path taken through the graph.\"\"\"\n",
    "    path = history['path']\n",
    "    costs = history['costs']\n",
    "    \n",
    "    # Create directed graph\n",
    "    G = nx.DiGraph()\n",
    "    for i in range(model.config.n_nodes):\n",
    "        G.add_node(i)\n",
    "    \n",
    "    # Add all edges\n",
    "    for i in range(model.config.n_nodes):\n",
    "        for j in range(model.config.n_nodes):\n",
    "            if model.adjacency[i, j]:\n",
    "                G.add_edge(i, j)\n",
    "    \n",
    "    # Layout\n",
    "    pos = nx.spring_layout(G, seed=42, k=2, iterations=50)\n",
    "    \n",
    "    # Draw\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "    \n",
    "    # Color nodes: visited (yellow), start (green), target (red), others (lightgray)\n",
    "    visited_nodes = set(path)\n",
    "    node_colors = []\n",
    "    for i in range(model.config.n_nodes):\n",
    "        if i == 0:\n",
    "            node_colors.append('lightgreen')\n",
    "        elif i == model.target_node:\n",
    "            node_colors.append('lightcoral')\n",
    "        elif i in visited_nodes:\n",
    "            node_colors.append('gold')\n",
    "        else:\n",
    "            node_colors.append('lightgray')\n",
    "    \n",
    "    # Draw all nodes\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                           node_size=800, ax=ax, alpha=0.6)\n",
    "    \n",
    "    # Draw all edges (faint)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='lightgray', \n",
    "                           arrows=True, arrowsize=15, \n",
    "                           arrowstyle='->', ax=ax, alpha=0.2,\n",
    "                           connectionstyle='arc3,rad=0.1')\n",
    "    \n",
    "    # Draw path edges (bold)\n",
    "    path_edges = [(path[i], path[i+1]) for i in range(len(path)-1)]\n",
    "    nx.draw_networkx_edges(G, pos, edgelist=path_edges, \n",
    "                           edge_color='red', width=3,\n",
    "                           arrows=True, arrowsize=25, \n",
    "                           arrowstyle='->', ax=ax, alpha=0.8,\n",
    "                           connectionstyle='arc3,rad=0.1')\n",
    "    \n",
    "    # Draw labels\n",
    "    nx.draw_networkx_labels(G, pos, font_size=14, font_weight='bold', ax=ax)\n",
    "    \n",
    "    # Draw edge labels for path costs\n",
    "    edge_labels = {(path[i], path[i+1]): f\"{costs[i]:.1f}\" \n",
    "                   for i in range(len(costs))}\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=10, \n",
    "                                 font_color='red', ax=ax)\n",
    "    \n",
    "    total_cost = sum(costs)\n",
    "    ax.set_title(f'Path Taken (Total Cost: {total_cost:.2f})', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Legend\n",
    "    from matplotlib.patches import Patch\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Patch(facecolor='lightgreen', label='Start'),\n",
    "        Patch(facecolor='lightcoral', label='Target'),\n",
    "        Patch(facecolor='gold', label='Visited'),\n",
    "        Line2D([0], [0], color='red', linewidth=3, label='Path Taken')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper left', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_path(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Cost Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "steps = range(len(history['costs']))\n",
    "\n",
    "# Step costs\n",
    "ax1.bar(steps, history['costs'], color='steelblue', alpha=0.7)\n",
    "ax1.set_title('Edge Costs Per Step', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Step')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative cost\n",
    "ax2.plot(steps, history['cumulative_cost'], 'o-', linewidth=2, \n",
    "         color='darkgreen', markersize=6)\n",
    "ax2.set_title('Cumulative Cost', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Step')\n",
    "ax2.set_ylabel('Total Cost')\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Summary:\")\n",
    "print(f\"  Steps taken: {len(history['costs'])}\")\n",
    "print(f\"  Average cost per step: {np.mean(history['costs']):.2f}\")\n",
    "print(f\"  Total cost: {sum(history['costs']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Comparison\n",
    "\n",
    "Let's compare the three policies over multiple episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {\n",
    "    'Lookahead (\u03b8=0.5)': lookahead_policy,\n",
    "    'Greedy': greedy_policy,\n",
    "    'Random': random_policy,\n",
    "}\n",
    "\n",
    "# Run multiple episodes for each policy\n",
    "n_episodes = 20\n",
    "results = {}\n",
    "\n",
    "for name, policy in policies.items():\n",
    "    print(f\"\\nRunning {name} policy ({n_episodes} episodes)...\")\n",
    "    episode_results = []\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        key = jax.random.PRNGKey(i)  # Different seed per episode\n",
    "        history = run_episode(model, policy, max_steps=50, key=key, verbose=False)\n",
    "        \n",
    "        # Check if reached target\n",
    "        reached_target = (history['path'][-1] == model.target_node)\n",
    "        total_cost = sum(history['costs']) if history['costs'] else float('inf')\n",
    "        steps = len(history['costs'])\n",
    "        \n",
    "        episode_results.append({\n",
    "            'reached': reached_target,\n",
    "            'cost': total_cost,\n",
    "            'steps': steps,\n",
    "        })\n",
    "    \n",
    "    results[name] = episode_results\n",
    "    \n",
    "    # Summary\n",
    "    success_rate = sum(r['reached'] for r in episode_results) / n_episodes\n",
    "    successful_costs = [r['cost'] for r in episode_results if r['reached']]\n",
    "    avg_cost = np.mean(successful_costs) if successful_costs else float('inf')\n",
    "    avg_steps = np.mean([r['steps'] for r in episode_results if r['reached']])\n",
    "    \n",
    "    print(f\"  Success rate: {success_rate:.1%}\")\n",
    "    print(f\"  Avg cost (successful): {avg_cost:.2f}\")\n",
    "    print(f\"  Avg steps (successful): {avg_steps:.1f}\")\n",
    "\n",
    "# Compare statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Policy Comparison Summary\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Policy':<20} {'Success Rate':>15} {'Avg Cost':>12} {'Avg Steps':>12}\")\n",
    "print(\"-\"*60)\n",
    "for name, episode_results in results.items():\n",
    "    success_rate = sum(r['reached'] for r in episode_results) / n_episodes\n",
    "    successful_costs = [r['cost'] for r in episode_results if r['reached']]\n",
    "    avg_cost = np.mean(successful_costs) if successful_costs else float('inf')\n",
    "    successful_steps = [r['steps'] for r in episode_results if r['reached']]\n",
    "    avg_steps = np.mean(successful_steps) if successful_steps else float('inf')\n",
    "    \n",
    "    print(f\"{name:<20} {success_rate:>14.1%} {avg_cost:>12.2f} {avg_steps:>12.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Policy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "policy_names = list(results.keys())\n",
    "colors = ['steelblue', 'coral', 'lightgreen']\n",
    "\n",
    "# Success rates\n",
    "success_rates = [sum(r['reached'] for r in results[name]) / n_episodes \n",
    "                 for name in policy_names]\n",
    "axes[0].bar(policy_names, success_rates, color=colors, alpha=0.7)\n",
    "axes[0].set_title('Success Rate', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Proportion')\n",
    "axes[0].set_ylim([0, 1.1])\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(success_rates):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.1%}', ha='center', fontweight='bold')\n",
    "\n",
    "# Average costs (successful episodes only)\n",
    "avg_costs = []\n",
    "for name in policy_names:\n",
    "    successful_costs = [r['cost'] for r in results[name] if r['reached']]\n",
    "    avg_costs.append(np.mean(successful_costs) if successful_costs else 0)\n",
    "\n",
    "axes[1].bar(policy_names, avg_costs, color=colors, alpha=0.7)\n",
    "axes[1].set_title('Average Total Cost\\n(Successful Episodes)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Cost')\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(avg_costs):\n",
    "    if v > 0:\n",
    "        axes[1].text(i, v + max(avg_costs)*0.02, f'{v:.1f}', \n",
    "                    ha='center', fontweight='bold')\n",
    "\n",
    "# Average steps (successful episodes only)\n",
    "avg_steps = []\n",
    "for name in policy_names:\n",
    "    successful_steps = [r['steps'] for r in results[name] if r['reached']]\n",
    "    avg_steps.append(np.mean(successful_steps) if successful_steps else 0)\n",
    "\n",
    "axes[2].bar(policy_names, avg_steps, color=colors, alpha=0.7)\n",
    "axes[2].set_title('Average Steps to Target\\n(Successful Episodes)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel('Steps')\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(avg_steps):\n",
    "    if v > 0:\n",
    "        axes[2].text(i, v + max(avg_steps)*0.02, f'{v:.1f}', \n",
    "                    ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Costs Across Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Box plot of costs for successful episodes\n",
    "data_to_plot = []\n",
    "labels = []\n",
    "for name in policy_names:\n",
    "    successful_costs = [r['cost'] for r in results[name] if r['reached']]\n",
    "    if successful_costs:\n",
    "        data_to_plot.append(successful_costs)\n",
    "        labels.append(name)\n",
    "\n",
    "bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True,\n",
    "                showmeans=True, meanline=True)\n",
    "\n",
    "# Color boxes\n",
    "for patch, color in zip(bp['boxes'], colors[:len(data_to_plot)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax.set_title('Distribution of Total Costs (Successful Episodes)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Total Cost', fontsize=12)\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], color='orange', linewidth=2, linestyle='--', label='Median'),\n",
    "    Line2D([0], [0], color='green', linewidth=2, linestyle='-', label='Mean')\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "The simulation demonstrates:\n",
    "\n",
    "1. **LookaheadPolicy with backward induction** typically performs best by:\n",
    "   - Planning multiple steps ahead using dynamic programming\n",
    "   - Using learned cost estimates to make informed decisions\n",
    "   - Computing optimal value functions over the lookahead horizon\n",
    "\n",
    "2. **Learning dynamics**:\n",
    "   - The agent improves cost estimates through running averages\n",
    "   - Early decisions may be suboptimal due to uncertain estimates\n",
    "   - Later decisions benefit from accumulated knowledge\n",
    "\n",
    "3. **Risk sensitivity** (via \u03b8 parameter):\n",
    "   - \u03b8 = 0.0: Pessimistic, plans for worst-case costs\n",
    "   - \u03b8 = 0.5: Risk-neutral, uses expected costs (demonstrated here)\n",
    "   - \u03b8 = 1.0: Optimistic, assumes best-case costs\n",
    "\n",
    "4. **Planning horizon tradeoffs**:\n",
    "   - Longer horizons provide better decisions but higher computation\n",
    "   - Backward induction ensures time-consistent optimal paths\n",
    "   - Greedy (1-step) policy is fast but may miss better long-term paths\n",
    "\n",
    "5. **Graph structure impact**:\n",
    "   - Dense graphs (high edge_prob) offer more route options\n",
    "   - Sparse graphs may force suboptimal paths\n",
    "   - Cost variance (spread) affects risk-sensitive decisions\n",
    "\n",
    "---\n",
    "\n",
    "## Extensions\n",
    "\n",
    "Try modifying:\n",
    "- `horizon`: See how lookahead depth affects performance\n",
    "- `theta`: Test risk-averse (\u03b8=0.2) vs. risk-seeking (\u03b8=0.8) behavior\n",
    "- `edge_prob`: Create denser or sparser graphs\n",
    "- `max_spread`: Increase uncertainty to see adaptation\n",
    "- `n_nodes`: Scale to larger graphs\n",
    "- Implement your own policy using learned costs!\n",
    "\n",
    "## References\n",
    "\n",
    "- Repository: https://github.com/pedronahum/stochastic-optimization\n",
    "- JAX Documentation: https://jax.readthedocs.io/\n",
    "- Stochastic Shortest Path: Bertsekas, D. P. (2012). Dynamic Programming and Optimal Control, Vol. II\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}